
# 原则

## 没有免费的午餐

- 没有任何一种算法对所有问题都有效，在监督学习（即预测建模）中尤其如此。

## 大原则

- 所有机器学习算法预测建模的基础
  > 机器学习算法被描述为学习一个目标函数${f}$，该函数将输入变量${ X }$最好地映射到输出变量${Y：Y = f(X)}$


# 1.线性回归
## 1.1 简介
- 线性回归的表示是一个方程，它通过找到输入变量的特定权重（称为系数 B），来描述一条最适合表示输入变量 x 与输出变量 y 关系的直线。


# 2.逻辑回归

## 2.1 Logistic 概述
Logistic 回归是机器学习从统计学中借鉴的另一种技术。它是解决二分类问题的首选方法。

逻辑回归(LogisticRegression)简单来看就是在线性回归模型外面再套了一个$Sigmoid$函数：  \n",
    "$$\n",
    "\\delta(t)=\\frac{1}{1+e^{-t}}\n",
    "$$\n",   


Logistic 回归与线性回归相似，目标都是找到每个输入变量的权重，即系数值。与线性回归不同的是，Logistic 回归对输出的预测使用被称为 logistic 函数的非线性函数进行变换。


# 3.线性判别分析

## 3.1 简介
Logistic 回归是一种分类算法，传统上，它仅限于只有两类的分类问题。如果你有两个以上的类别，那么线性判别分析是首选的线性分类技术。

LDA 的表示非常简单直接。它由数据的统计属性构成，对每个类别进行计算。单个输入变量的 LDA 包括：
- 每个类别的平均值；
- 所有类别的方差。
![LDA](https://image.jiqizhixin.com/uploads/editor/ae1b0ae8-02f4-45f4-8e2f-7e8fd091732f/46292640-3.jpeg)

**进行预测的方法**是计算每个类别的判别值并对具备最大值的类别进行预测。

LDA假设数据呈高斯分布（钟形曲线），因此最好预先从数据中删除异常值。这是处理分类预测建模问题的一种简单而强大的方法。

# 4.分类和回归树

决策树是预测建模机器学习的一种重要算法。

决策树模型的表示是一个二叉树。这是算法和数据结构中的二叉树，没什么特别的。每个节点代表一个单独的输入变量 x 和该变量上的一个分割点（假设变量是数字）。

![决策树](https://image.jiqizhixin.com/uploads/editor/e488ceae-30b5-495b-84e3-2e47ea0629d6/99736640-4.jpeg)

决策树的叶节点包含一个用于预测的输出变量 y。通过遍历该树的分割点，直到到达一个叶节点并输出该节点的类别值就可以作出预测。

决策树学习速度和预测速度都很快。它们还可以解决大量问题，并且不需要对数据做特别准备。

# 5.朴素贝叶斯

> 小周老师不推荐

## 5.1 简介

朴素贝叶斯是一个简单但是很强大的预测建模算法

模型由两种概率组成，而且两种模型都可以直接从训练数据中计算出来
- 每个类别的概率；
- 给定每个 x 的值，每个类别的条件概率

![贝叶斯定理](https://image.jiqizhixin.com/uploads/editor/3335b686-f2e8-4691-9f36-e1bec3a3bf0e/34576640-5.jpeg)

朴素贝叶斯之所以是朴素的，是因为它假设每个输入变量是独立的。这是一个强大的假设，真实的数据并非如此，但是，该技术在大量复杂问题上非常有用。


# 6.K-最近邻 (KNN)

## 6.1 简介

KNN 算法非常简单且有效。KNN 的模型表示是整个训练数据集。

KNN 算法在整个训练集中搜索 K 个最相似实例（近邻）并汇总这 K 个实例的输出变量，以预测新数据点。
- 对于回归问题，这可能是平均输出变量；
- 对于分类问题，这可能是众数（或最常见的）类别值

![KNN](https://image.jiqizhixin.com/uploads/editor/fe51e19d-58e1-4284-b0d0-ec1895d650b8/97037640-6.jpeg)

特点：

诀窍在于如何确定数据实例间的相似性。如果属性的度量单位相同（例如都是用英寸表示），那么最简单的技术是使用欧几里得距离，你可以根据每个输入变量之间的差值直接计算出来其数值。

KNN 需要大量内存或空间来存储所有数据，但是只有在需要预测时才执行计算（或学习）。

距离或紧密性的概念可能在非常高的维度（很多输入变量）中会瓦解，这对算法在你的问题上的性能产生负面影响。这被称为维数灾难。因此你最好只使用那些与预测输出变量最相关的输入变量。

# 7.学习向量量化 (LVQ)

## 7.1 简介

K 近邻算法的一个缺点是你需要遍历整个训练数据集。学习向量量化算法（简称 LVQ）是一种人工神经网络算法，它允许你选择训练实例的数量，并精确地学习这些实例应该是什么样的。

![LVQ](https://image.jiqizhixin.com/uploads/editor/2637601c-abd4-495c-b1ab-d40c761caa4b/56330640-7.jpeg)

LVQ 的表示是码本向量的集合。这些是在开始时随机选择的，并逐渐调整以在学习算法的多次迭代中最好地总结训练数据集。

在学习之后，码本向量可用于预测（类似 K 近邻算法）。最相似的近邻（最佳匹配的码本向量）通过计算每个码本向量和新数据实例之间的距离找到。然后返回最佳匹配单元的类别值或（回归中的实际值）作为预测。

如果你重新调整数据，使其具有相同的范围（比如 0 到 1 之间），就可以获得最佳结果。

如果你发现 KNN 在你的数据集上达到很好的结果，请尝试用 LVQ 减少存储整个训练数据集的内存要求。


# 8.支持向量机 (SVM)

## 8.1 简介
支持向量机可能是最受欢迎和最广泛讨论的机器学习算法之一。

超平面是分割输入变量空间的一条线。

在 SVM 中，选择一条可以最好地根据输入变量类别（类别 0 或类别 1）对输入变量空间进行分割的超平面。

在二维中，你可以将其视为一条线，我们假设所有的输入点都可以被这条线完全的分开。SVM 学习算法找到了可以让超平面对类别进行最佳分割的系数。
![支持向量机](https://image.jiqizhixin.com/uploads/editor/21e3bd19-0be5-430d-9fbe-cf1244e284ab/60573640-8.jpeg)

超平面和最近的数据点之间的距离被称为间隔。分开两个类别的最好的或最理想的超平面具备最大间隔。只有这些点与定义超平面和构建分类器有关。这些点被称为支持向量，它们支持或定义了超平面。实际上，优化算法用于寻找最大化间隔的系数的值。

# 9.Bagging 和随机森林

## 9.1 简介

随机森林是最流行和最强大的机器学习算法之一。它是 Bootstrap Aggregation（又称 bagging）集成机器学习算法的一种。

bootstrap 是从数据样本中估算数量的一种强大的统计方法。例如平均数。你从数据中抽取大量样本，计算平均值，然后平均所有的平均值以便更好的估计真实的平均值。

### Bagging

bagging 使用相同的方法，但是它估计整个统计模型，最常见的是决策树。在训练数据中抽取多个样本，然后对每个数据样本建模。当你需要对新数据进行预测时，每个模型都进行预测，并将所有的预测值平均以便更好的估计真实的输出值。

![随机森林](https://image.jiqizhixin.com/uploads/editor/08544c7b-ffc1-44f2-992c-120628bbe9d9/02248640-9.jpeg)

随机森林是对这种方法的一种调整，在随机森林的方法中决策树被创建以便于通过引入随机性来进行次优分割，而不是选择最佳分割点。

因此，针对每个数据样本创建的模型将会与其他方式得到的有所不同，不过虽然方法独特且不同，它们仍然是准确的。结合它们的预测可以更好的估计真实的输出值。

如果你用方差较高的算法（如决策树）得到了很好的结果，那么通常可以通过 bagging 该算法来获得更好的结果。

# 10.Boosting 和 AdaBoost

## 10.1 简介
Boosting 是一种集成技术，它试图集成一些弱分类器来创建一个强分类器。这通过从训练数据中构建一个模型，然后创建第二个模型来尝试纠正第一个模型的错误来完成。一直添加模型直到能够完美预测训练集，或添加的模型数量已经达到最大数量。

AdaBoost 是第一个为二分类开发的真正成功的 boosting 算法。这是理解 boosting 的最佳起点。现代 boosting 方法建立在 AdaBoost 之上，最显著的是随机梯度提升。

![AdaBoost](https://image.jiqizhixin.com/uploads/editor/5bc1b087-0e9d-4a61-8e06-ebccc42162c4/39628640-11.jpeg)

AdaBoost 与短决策树一起使用。在第一个决策树创建之后，利用每个训练实例上树的性能来衡量下一个决策树应该对每个训练实例付出多少注意力。难以预测的训练数据被分配更多权重，而容易预测的数据分配的权重较少。依次创建模型，每个模型在训练实例上更新权重，影响序列中下一个决策树的学习。在所有决策树建立之后，对新数据进行预测，并且通过每个决策树在训练数据上的精确度评估其性能。

因为在纠正算法错误上投入了太多注意力，所以具备已删除异常值的干净数据非常重要。

# 总结
- 初学者在面对各种机器学习算法时经常问：「我应该用哪个算法？」
  - （1）数据的大小、质量和特性；
  - （2）可用的计算时间；
  - （3）任务的紧迫性；
  - （4）你想用这些数据做什么。




# reference
- [csdn-机器学习十大算法](https://blog.csdn.net/qq_39783601/article/details/123365469?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165625337216782184618561%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&request_id=165625337216782184618561&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-123365469-null-null.142^v24^huaweicloudv2,157^v15^new_3&utm_term=%E5%8D%81%E5%A4%A7%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95&spm=1018.2226.3001.4187)
- [机器之心-机器学习十大算法](https://www.jiqizhixin.com/articles/a-tour-of-the-top-10-algorithms-for-machine-learning-newbies)
