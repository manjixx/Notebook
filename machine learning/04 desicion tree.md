
# 4.决策树

## 4.1 基本概念

- 一般情况下复杂嵌套的if else规则可能存在的痛点：
  - 规则可能不完备，存在某些匹配不上的情况；
  - 规则之间存在冗余，多个if else情况其实是判断的同样的条件；
  - 严重时，可能会出现矛盾的情况，即相同的条件，即有放，又有不放；
  - 判断规则的优先级混乱，比如信贷情况因子可以优先考虑，因为只要它是非常好就可以放款，而不必先判断其它条件

- 决策树是预测建模机器学习的一种重要算法，它能保证所有的规则互斥且完备，即用户的任意一种情况一定能匹配上一条规则，且该规则唯一，这样就能解决上面的痛点1~3，且规则判断的优先级也很不错；

![决策树](https://image.jiqizhixin.com/uploads/editor/e488ceae-30b5-495b-84e3-2e47ea0629d6/99736640-4.jpeg)

- 决策树模型的表示是一个二叉树。每个节点代表一个单独的输入变量 x 和该变量上的一个分割点（假设变量是数字）；决策树的叶节点包含一个用于预测的输出变量 y。通过遍历该树的分割点，直到到达一个叶节点并输出该节点的类别值就可以作出预测。

## 4.2 决策树学习

- 决策树算法可以从已标记的数据中自动学习出if-else规则集，如图所示左侧为案例，右侧为决策树学习之后得到的决策树，其结构包括：

![决策树案例](https://nbviewer.org/github/zhulei227/ML_Notes/blob/master/notebooks/source/09_%E5%86%B3%E7%AD%96%E6%A0%91%E5%AD%A6%E4%B9%A0.jpg)

  - 有向边  

  - 节点
    - 叶子节点，叶子节点即表示实例的预测值(分类/回归)  
    - 非叶子节点，非叶子节点主要用于对某一特征做判断，而它下面所链接的有向边表示该特征所满足的某条件

- 决策树的学习主要分为两个阶段：
  - 决策树生成：该阶段最重要就是特征选择
  - 决策树剪枝  

### 1. 特征选择

特征选择用于选择对分类有用的特征，ID3和C4.5通常选择的准则是信息增益和信息增益比，下面对其作介绍并实现

#### 信息增益

- 两个随机变量之间的互信息公式

  ${MI(Y,X) = H(Y) - H(Y|X)}$

其中，${H(X}$表示${X}$的熵：

  ${H(X) = - \sum_{i = 1}^n p_i logp_i,其中 p_i = P(X = x_i)}$,
 
条件熵$H(Y|X)$表示在已知随机变量$X$的条件下，随机变量$Y$的不确定性

  ${H(Y｜X) = - \sum_{i = 1}^n p_i H(Y|X = x_i),其中 p_i = P(X = x_i)}$
  
信息增益就是Y取分类标签，X取某一特征时的**互信息**，它表示如果选择特征X对数据进行分割，可以使得分割后Y分布的熵降低多少，若降低的越多，说明分割每个子集的Y的分布越集中，则X对分类标签Y越有用

